{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import unittest\n",
    "from typing import List, Dict, Any\n",
    "import random\n",
    "import numpy as np\n",
    "from random import choices\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from lr_scheduler import NoamOpt\n",
    "from transformer import Transformer\n",
    "from vocabulary import Vocabulary\n",
    "from transformer_utils import construct_batches\n",
    "from train import train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-02T08:07:24.839458Z",
     "start_time": "2023-09-02T08:07:24.820456Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-02T08:08:09.566108Z",
     "start_time": "2023-09-02T08:07:56.145525Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus 6\n",
      "Number of batches 3\n",
      "epoch: 0, num_iters: 0, batch_loss: 13.952544212341309, batch_accuracy: 0.0\n",
      "epoch: 1, num_iters: 3, batch_loss: 12.951950073242188, batch_accuracy: 0.0\n",
      "epoch: 2, num_iters: 6, batch_loss: 10.813690185546875, batch_accuracy: 0.0\n",
      "epoch: 3, num_iters: 9, batch_loss: 7.460815906524658, batch_accuracy: 0.0\n",
      "epoch: 4, num_iters: 12, batch_loss: 4.79967737197876, batch_accuracy: 0.0416666679084301\n",
      "epoch: 5, num_iters: 15, batch_loss: 3.026728868484497, batch_accuracy: 0.2083333283662796\n",
      "epoch: 6, num_iters: 18, batch_loss: 2.1718480587005615, batch_accuracy: 0.125\n",
      "epoch: 7, num_iters: 21, batch_loss: 1.7464728355407715, batch_accuracy: 0.4583333432674408\n",
      "epoch: 8, num_iters: 24, batch_loss: 1.4861321449279785, batch_accuracy: 0.5\n",
      "epoch: 9, num_iters: 27, batch_loss: 1.1562992334365845, batch_accuracy: 0.5416666865348816\n",
      "epoch: 10, num_iters: 30, batch_loss: 1.1372474431991577, batch_accuracy: 0.5416666865348816\n",
      "epoch: 11, num_iters: 33, batch_loss: 1.0639647245407104, batch_accuracy: 0.7083333134651184\n",
      "epoch: 12, num_iters: 36, batch_loss: 1.028711199760437, batch_accuracy: 0.625\n",
      "epoch: 13, num_iters: 39, batch_loss: 0.8185713887214661, batch_accuracy: 0.7916666865348816\n",
      "epoch: 14, num_iters: 42, batch_loss: 0.7393266558647156, batch_accuracy: 0.75\n",
      "epoch: 15, num_iters: 45, batch_loss: 0.6575194001197815, batch_accuracy: 0.7916666865348816\n",
      "epoch: 16, num_iters: 48, batch_loss: 0.6175935864448547, batch_accuracy: 0.875\n",
      "epoch: 17, num_iters: 51, batch_loss: 0.45068061351776123, batch_accuracy: 0.9583333134651184\n",
      "epoch: 18, num_iters: 54, batch_loss: 0.2980586588382721, batch_accuracy: 0.9583333134651184\n",
      "epoch: 19, num_iters: 57, batch_loss: 0.25764986872673035, batch_accuracy: 0.9583333134651184\n",
      "epoch: 20, num_iters: 60, batch_loss: 0.15830381214618683, batch_accuracy: 0.9583333134651184\n",
      "epoch: 21, num_iters: 63, batch_loss: 0.07756254822015762, batch_accuracy: 1.0\n",
      "epoch: 22, num_iters: 66, batch_loss: 0.058144886046648026, batch_accuracy: 1.0\n",
      "epoch: 23, num_iters: 69, batch_loss: 0.020241105929017067, batch_accuracy: 1.0\n",
      "epoch: 24, num_iters: 72, batch_loss: 0.007366810459643602, batch_accuracy: 1.0\n",
      "epoch: 25, num_iters: 75, batch_loss: 0.005587074439972639, batch_accuracy: 1.0\n",
      "epoch: 26, num_iters: 78, batch_loss: 0.0050779711455106735, batch_accuracy: 1.0\n",
      "epoch: 27, num_iters: 81, batch_loss: 0.002890718402341008, batch_accuracy: 1.0\n",
      "epoch: 28, num_iters: 84, batch_loss: 0.0024206095840781927, batch_accuracy: 1.0\n",
      "epoch: 29, num_iters: 87, batch_loss: 0.002916384255513549, batch_accuracy: 1.0\n",
      "epoch: 30, num_iters: 90, batch_loss: 0.0010791957611218095, batch_accuracy: 1.0\n",
      "epoch: 31, num_iters: 93, batch_loss: 0.0008776508620940149, batch_accuracy: 1.0\n",
      "epoch: 32, num_iters: 96, batch_loss: 0.0006560695474036038, batch_accuracy: 1.0\n",
      "epoch: 33, num_iters: 99, batch_loss: 0.0008009983575902879, batch_accuracy: 1.0\n",
      "epoch: 34, num_iters: 102, batch_loss: 0.000985236489214003, batch_accuracy: 1.0\n",
      "epoch: 35, num_iters: 105, batch_loss: 0.0008412934257648885, batch_accuracy: 1.0\n",
      "epoch: 36, num_iters: 108, batch_loss: 0.0006800434202887118, batch_accuracy: 1.0\n",
      "epoch: 37, num_iters: 111, batch_loss: 0.0008914726204238832, batch_accuracy: 1.0\n",
      "epoch: 38, num_iters: 114, batch_loss: 0.00039923025178723037, batch_accuracy: 1.0\n",
      "epoch: 39, num_iters: 117, batch_loss: 0.0004572299949359149, batch_accuracy: 1.0\n",
      "epoch: 40, num_iters: 120, batch_loss: 0.00041903098463080823, batch_accuracy: 1.0\n",
      "epoch: 41, num_iters: 123, batch_loss: 0.0006093038246035576, batch_accuracy: 1.0\n",
      "epoch: 42, num_iters: 126, batch_loss: 0.0002748411789070815, batch_accuracy: 1.0\n",
      "epoch: 43, num_iters: 129, batch_loss: 0.00030008339672349393, batch_accuracy: 1.0\n",
      "epoch: 44, num_iters: 132, batch_loss: 0.0003457523707766086, batch_accuracy: 1.0\n",
      "epoch: 45, num_iters: 135, batch_loss: 0.0003323096316307783, batch_accuracy: 1.0\n",
      "epoch: 46, num_iters: 138, batch_loss: 0.00025424122577533126, batch_accuracy: 1.0\n",
      "epoch: 47, num_iters: 141, batch_loss: 0.00023125264851842076, batch_accuracy: 1.0\n",
      "epoch: 48, num_iters: 144, batch_loss: 0.00023373635485768318, batch_accuracy: 1.0\n",
      "epoch: 49, num_iters: 147, batch_loss: 0.00023210421204566956, batch_accuracy: 1.0\n",
      "batch loss 0.00011893750343006104\n",
      "batch accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "synthetic_corpus_size = 5\n",
    "batch_size = 2\n",
    "n_epochs = 40\n",
    "n_tokens_in_batch = 10\n",
    "\n",
    "# Construct vocabulary and create synthetic data by uniform randomly sampling tokens from it\n",
    "# Note: the original paper uses byte pair encodings, we simply take each word to be a token.\n",
    "corpus = [\"These are the tokens that will end up in our vocabulary\"]\n",
    "vocab = Vocabulary(corpus)\n",
    "vocab_size = len(\n",
    "    list(vocab.token2index.keys())\n",
    ")  # 14 tokens including bos, eos and pad\n",
    "valid_tokens = list(vocab.token2index.keys())[3:]\n",
    "corpus += [\n",
    "    \" \".join(choices(valid_tokens, k=n_tokens_in_batch))\n",
    "    for _ in range(synthetic_corpus_size)\n",
    "]\n",
    "print(f\"corpus {len(corpus)}\")\n",
    "\n",
    "# Construct src-tgt aligned input batches (note: the original paper uses dynamic batching based on tokens)\n",
    "corpus = [{\"src\": sent, \"tgt\": sent} for sent in corpus]\n",
    "batches, masks = construct_batches(\n",
    "    corpus,\n",
    "    vocab,\n",
    "    batch_size=batch_size,\n",
    "    src_lang_key=\"src\",\n",
    "    tgt_lang_key=\"tgt\",\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(f\"Number of batches {len(batches['src'])}\")\n",
    "\n",
    "# Initialize transformer\n",
    "transformer = Transformer(\n",
    "    hidden_dim=512,\n",
    "    ff_dim=2048,\n",
    "    num_heads=8,\n",
    "    num_layers=2,\n",
    "    max_decoding_length=25,\n",
    "    vocab_size=vocab_size,\n",
    "    padding_idx=vocab.token2index[vocab.PAD],\n",
    "    bos_idx=vocab.token2index[vocab.BOS],\n",
    "    dropout_p=0.1,\n",
    "    tie_output_to_embedding=True,\n",
    ").to(device)\n",
    "\n",
    "# Initialize learning rate scheduler, optimizer and loss (note: the original paper uses label smoothing)\n",
    "optimizer = torch.optim.Adam(\n",
    "    transformer.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9\n",
    ")\n",
    "scheduler = NoamOpt(\n",
    "    transformer.hidden_dim,\n",
    "    factor=1,\n",
    "    warmup=400,\n",
    "    optimizer=optimizer,\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Start training and verify ~zero loss and >90% accuracy on the last batch\n",
    "latest_batch_loss, latest_batch_accuracy = train(\n",
    "    transformer, scheduler, criterion, batches, masks, n_epochs=n_epochs\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"batch loss {latest_batch_loss.item()}\")\n",
    "print(f\"batch accuracy {latest_batch_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
